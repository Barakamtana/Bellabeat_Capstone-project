---
title: "Bellabeat_Capstone_Project"
author: "Baraka Mtana"
date: "`r Sys.Date()`"
output:
  pdf_document:
    keep_tex: true
    number_sections: true
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Important shortcuts we'll reuse
#### Restart r = "Ctrl + Shift + F10"
#### Add <- = "Alt + i"
#### %>%  = "Ctrl + Shift + M"
#### New r line code =  "Ctrl + Alt + I"

```{r}
#options(repos = c(CRAN = "https://cloud.r-project.org/"))
# Set up environment
# import libraries
library(tidyverse)
# install.packages("lubridate")
library(lubridate)
library(ggplot2)
library(dplyr)

#install.packages("plotly")
library(plotly)

```


```{r}
# get and print working directory
currentDir <- getwd() 
print(currentDir)

# list file in the working DIR
list.files(currentDir)

# we are interested in the csv files in 'mturkfitbit_export_3.12.16-4.11.16'
# and 'Fitabase Data 3.12.16-4.11.16'
csv_files_Dir <- file.path(
  currentDir, 'mturkfitbit_export_3.12.16-4.11.16', 'Fitabase Data 3.12.16-4.11.16'
)

csv_files <- list.files(csv_files_Dir)
len_cvs = length(csv_files)

# csv_files <- list.files(csv_files_Dir, pattern = "\\.csv$", full.names = TRUE)
# print(csv_files)
```

```{r}
# Here we'll write function we'll reuse

# Check for missing values
missing_value <- function(df, file){
  
  # Initialize an empty list that will be a key value pair
  # It will hold df_name with missing values and the specific column with the missing values
  df_with_missing_values <- list()
  
  if (sum(is.na(df)) == 0){
    
    print(paste(file, "Has no missing values"))
  
  }else if (sum(is.na(df)) > 0){
    
    
    #append df_name with missing vales to df_with_missing_values
    df_with_missing_values[[file]] <- c() # initialize an empty vector to store
     #column names
    
    
    #get df columns with missing values
    colNames = colnames(df)
    
    for (col in colNames){
      if (sum(is.na(df[[col]]))  > 0){
  
        df_with_missing_values[[file]] <- c(df_with_missing_values[[file]], col)
        
        print(paste("Total missing values in", file, col, "is",
                    sum(is.na(df[col]))))
      }
    }
    
  }
  
  return(df_with_missing_values)
}



# get number of unique values
unique_value <- function(df, column_of_interest) {
  #get the unique values
  uniques <- unique(df[[column_of_interest]])
  
  # Get the unique values from the specified column
  n_unique <- length(uniques)
  
  print(paste(column_of_interest, "has",  n_unique, "values"))
  
}


# Let's check if we are dealing with the same clients
uniqueIDs_Comparison <- function (df1, column1, df2, column2){
  missing_ids = c()
  
  initial_list = unique(df1[[column1]])
  comparison_list = unique(df2[[column2]])
  
  
  for (i in initial_list){
    if (!(i %in% comparison_list)){
      missing_ids <- c(missing_ids, i)
      
    }
  }
  
  for (i in comparison_list){
    if (!(i %in% initial_list)){
      missing_ids <- c(missing_ids, i)
      
    }
    
  }
  
  if (length(missing_ids) == 0){
    print("Dataframes have similar values")
    
    
  }else{
    print("Dataframes have different values")  
    print(paste("No. of missing values", length(unique(missing_ids))))
  }
  
  return(unique(missing_ids))
}


# Get the length of values in a column
N_unique_char <- function(df, column_of_interest){
  # Initialize/ pre-allocate a numeric vector of a specific length, initialized with   zeros
  n_char <- numeric(nrow(df)) 
  
  for (i in seq_along(df[[column_of_interest]])){
    n_char[i] <- nchar(df[[column_of_interest]][i])
  }
  
  # get the number of unique characters
  character_lens <- unique(n_char)
  
  return(character_lens)
}

  

change_to_dateTime <- function(df, column_of_interest){
  
  Values_Ncha_Column = N_unique_char(df, column_of_interest)
  
  #Check if all elements in the vector Values_Ncha_Column are either 8 or 9
  if (all(Values_Ncha_Column %in% c(8, 9))){
    df[[column_of_interest]] <- 
      as.POSIXct(
        df[[column_of_interest]],
        format="%m/%d/%Y",
        tz=Sys.timezone()
    )
    
  }else if (!all(Values_Ncha_Column
                 %in% c(8, 9))){
    
    df[[column_of_interest]] <- 
      as.POSIXct(
        df[[column_of_interest]],
        # %I: Hour (01â€“12, for 12-hour clock)
        # %p: AM/PM marker
        format = "%m/%d/%Y %I:%M:%S %p", 
        tz=Sys.timezone()
    )
  }
  
  # confirm the datatype of the column of interest
  #print(paste(column_of_interest, "has" ,class(df[[column_of_interest]]),
              #"datatype"))
  
  print(head(df[column_of_interest], 5))
  print(tail(df[column_of_interest], 5))
  
  return(df)
  
}



# Check if the columns are identical
identical_columns <- function(df, col1, col2){
  if (identical(df[[col1]], df[[col2]])) {
    print("The columns are identical.")
  } else {
    print("The columns are NOT identical.")
  } 
}



# Merge fucntion which we'll use with the reduce inbuilt function
merge_dfs <- function(df1, df2){
  merged_df <- merge(df1, df2, by = "Id")
  
  return(merged_df)
}

# calculate column averages
Avgs <- function(df, column_names_vector){
  
  # initialize and empty list
  column_Avgs = list()
  
  for (i in column_names_vector){
    # get the data of the column
    column_data <- df[, c("Id", i)]

    
    avg_per_athlete <- column_data %>% 
      group_by(Id) %>% 
      summarise(
        # use "get" to retrieves the variable from key which is i and 
        # Rename the column_Avgs keys to avoid conflicts
        !!paste0("Avg_", i) := mean(get(i), na.rm = TRUE) 
      )
    
    
    column_Avgs[[i]] = avg_per_athlete
    

  }
  
  # Merge DataFrames on the 'ID' column
  # How the reduce function works
  # 1.Reduce will take the first two data frames from column_Avgs and
  #pass them to merge_dfs.
  # 2. The result of the first merge will be passed as the first argument
  # to merge_dfs for the next data frame in the list, continuing this process until
  #all data frames have been merged.

  # Here the reduce function and
  merged_df <- Reduce(merge_dfs, column_Avgs)

  return(merged_df)
}


# line plot function 
line_plot <- function(df, x_col, y_col){
  plot <- ggplot (data = df) +
    geom_line(mapping = aes(x= .data[[x_col]] , y= .data[[y_col]] )) +
    labs(title = paste(x_col, "and", y_col, "relationsip"))
  
  print(plot)
  
}

weekDate_grouping <- function(df, weekOrdate){
  # get group by the 1st 2 columns which will be "Id and column with POSIXct
  #datatype"
  grouping_Cols <- c(colnames(df)[1:2])
  summarise_Cols <- c(colnames(df)[-1:-2])
  
  # Check if the first column is "Id" and the second column is of type POSIXct
  if (grouping_Cols[1] == "Id" && inherits(df[[grouping_Cols[2]]], "POSIXct")){
    
    
    # Determine grouping based on the 'weekOrdate' parameter
    if(weekOrdate == "week"){
      # Group by week number
      df <- df %>%
        mutate(Group = format(.data[[grouping_Cols[2]]], "%Y-%U"))
    }else if(weekOrdate == "date"){
      df <- df %>% 
        mutate(Group = as.Date(.data[[grouping_Cols[2]]]))
    }else {
      stop("Invalid string value for 'weekOrdate'. Use 'week' or 'date'.")
    }
    
    # If the above run successfully perform grouping and summarization
    df <- df %>%
      group_by(.data[[grouping_Cols[1]]], Group) %>%
      summarize(
        across(
          all_of(summarise_Cols),
          list(
            Total = ~sum(.x, na.rm = TRUE),
            Avg = ~mean(.x, na.rm = TRUE)
          ),
          #{col} acts as a Placeholder for the name of the original column being
          #processed
          #{fn}: Placeholder for the name of the function applied (e.g., "Total"
          #or "Avg").
          .names = "{col}_{fn}"
        ),
        .groups = "drop"
      )
    
  }else{
    stop("Please check the name and datatype of the 1st 2 columns.")
  }
  
  return(df)
}

```



```{r}
# Initialize an empty list to store data frames
dfs <- list()


for (file in csv_files) {
  
  print(paste("Working on", file))
  
  # create df names
  # split the file name str character
  df_name <- strsplit(file, split = '\\.')[[1]] #Access the first and only string
  
  # get the first part of the string character which is basically the name 
  # without the csv extension
  df_name <- df_name[1]
  
  # concatenate the df_name with df
  df_name <- paste0(df_name, "_df") # Use paste0 for no space between parts
  
  # create full path for each file so that we can import them
  filepath <- file.path(csv_files_Dir, file)
  
  # read csv 
  df <- read.csv(filepath)
  
  #Check if df has missing value
  missing_value(df = df, file = file)
  
  # append dfs and their names
  dfs[[df_name]] <- df  # Store the data frame in the list, keyed by file path

}


```


```{r}
# Confirm that all df were read successfully
if (len_cvs == length(dfs)) {
  print("All files read successfully")
} else {
  print("Some files were not read correctly")
}

```
```{r}
print(names(dfs))
```




```{r}
dailyActivity_merged_df <-dfs[["dailyActivity_merged_df"]]
str(dailyActivity_merged_df)

# we'll have a look at all unique value in if non of them is repeated
# change ActivityDate from character to datetime
# Assuming TotalSteps is cadence we'll see the average length of a step per person
# See if TotalDistance and TrackerDistance distance record the same data
# Calculate average TotalDistance, TrackerDistance, VeryActiveDistance, 
# ModeratelyActiveDistance, LightActiveDistance, SedentaryActiveDistance,
# Average calories lost per day
```

check how many customers are we dealing with
```{r}
# call function on dailyActivity_merged_df
unique_value(dailyActivity_merged_df, "Id")
```

See the number of character in each character of the ActivityDate column 
since we've already seen there are inconsistencies in the ActivityDate, -->
let's see if there are some date characters saved with hrs,min,and secs


```{r}
# see if there is uniformity in the Activity date column
N_unique_char(dailyActivity_merged_df, "ActivityDate")
```

```{r}
# call change_to_date on dailyActivity_merged_df
dailyActivity_merged_df <- change_to_dateTime(dailyActivity_merged_df,
                                              "ActivityDate")
```

```{r}
# call identical_columns on dailyActivity_merged_df
identical_columns(dailyActivity_merged_df, "TotalDistance", "TrackerDistance" )
```

```{r}
# Call the Avgs function to get the averages of some columns in dailyActivity_merged_df

column_names <- c(names(dailyActivity_merged_df)[3:10], tail(names(dailyActivity_merged_df), 1))

averages_df = Avgs(dailyActivity_merged_df, column_names)

# Write the data frame to a CSV file
# write.csv(averages_df, file = "customer_averages.csv", row.names = FALSE)


```

```{r}
head(averages_df, 5)
```


Check the correlation between TotalDistance and Calories
```{r}
# call function on dailyActivity_merged_df 
line_plot(df = dailyActivity_merged_df, x_col= "Calories", y_col = "TotalDistance")
```
There is a positive correlation between Calories and TotalDistance the is 


## Let's get to see heartrate_seconds_merged_df

```{r}
heartrate_seconds_merged_df <- dfs[["heartrate_seconds_merged_df"]]
str(heartrate_seconds_merged_df)

head(heartrate_seconds_merged_df, 5)
tail(heartrate_seconds_merged_df, 5)
# Check if the unique Ids are similar to the ones in dailyActivity_merged_df
# change time column to datetime
# see the average heart rate per unique id, see the correlation between the average heart rate and calories lost
# see if there is any negative hr values
```


Confirm if we are dealing with the same atheletes.
Check if the unique Ids are similar to the ones in dailyActivity_merged_df
```{r}
IDs <- uniqueIDs_Comparison(df1 = dailyActivity_merged_df, column1 = "Id", df2 = heartrate_seconds_merged_df, column2 = "Id")
  
```
These means we we are only dealing with 14 clients in both dailyActivity_merged_df, and heartrate_seconds_merged_df dataframes.
```{r}
print(IDs)
```

Change the time columns to datetime
```{r}
heartrate_seconds_merged_df <- change_to_dateTime(heartrate_seconds_merged_df, "Time")
```



see the average heart rate per unique id, see the correlation between the average heart rate and calories lost
```{r}
# calculate averages
average_hr <- function(df, column_calculate_avg){
  averages_hr_per_athlete <- df %>%
    group_by(Id)  %>%
      summarise(
        Avg_Hr = mean(column_calculate_avg),
    )
  
  print(head(averages_per_athlete, 5))
}
```



```{r}

average_hr_per_athlete <- Avgs(heartrate_seconds_merged_df, c(names(heartrate_seconds_merged_df)[3]))
  
  
new_col_name = names(average_hr_per_athlete)[2]

print(average_hr_per_athlete)
print(max(average_hr_per_athlete[[new_col_name]]))  
print(min(average_hr_per_athlete[[new_col_name]])) 
print(median(ceiling(average_hr_per_athlete[[new_col_name]]), na.rm = TRUE))
(mean(ceiling(average_hr_per_athlete[[new_col_name]])))
```



```{r}
# Increase bottom margin to fit x-axis labels better
par(mar = c(8, 5, 4, 2) + 0.1)  


boxplot(Value ~ Id,
        data = heartrate_seconds_merged_df,
        main =" Different boxplots for each athlete's heartrate",
        xlab = " ",
        ylab = "Hearteate Frequency",
        col = "orange",
        #main = "Perpendicular",
        las = 2 
        #horizontal = TRUE
        #border="brown"
)

```

From the above plot we can see that we have huge outliers with some of the athletes having heart rate close to 200 and some close to almost 0


#### Here we'll analyse hourlyCalories_merged_df
```{r}
hourlyCalories_merged_df <- dfs[['hourlyCalories_merged_df']]
str(hourlyCalories_merged_df)

# Check if the unique Ids are similar to the ones in heartrate_seconds_merged_df
# see average calories lost each hour.
# Check which our are people most active
# change ActivityHour to datetime
```

```{r}
# Check if the unique Ids are similar to the ones in dailyActivity_merged_df
uniqueIDs_Comparison(dailyActivity_merged_df, "Id",  hourlyCalories_merged_df, "Id")

```



```{r}
# Change to datetime
hourlyCalories_merged_df <-  change_to_dateTime(hourlyCalories_merged_df, "ActivityHour")
```


```{r}
hourlyIntensities_merged_df <- dfs[["hourlyIntensities_merged_df"]]
str(hourlyIntensities_merged_df)

# Check if the unique Ids are similar to the ones in dailyActivity_merged_df 
# and hourlyCalories_merged_df and hourlyCalories_merged_df since they have similar number row
# merge with hourlyCalories_merged_df see intensity and calories correlation on the same plot
# see AverageIntensity and Calories corelaion

```
```{r}
# See head and tail
head(hourlyIntensities_merged_df, 5)
#print(tail(hourlyIntensities_merged_df), 5)
```


```{r}
column1 = "Id"
column2 = "Id"
uniqueIDs_Comparison(hourlyCalories_merged_df, column1,
                     hourlyIntensities_merged_df,
                     column2)

```
```{r}
hourlyIntensities_merged_df <- change_to_dateTime(hourlyIntensities_merged_df, "ActivityHour")
```


```{r}
# confirm that the ActivityHour in both hourlyCalories_merged_df and hourlyIntensities_merged_df are similar
column1 = "ActivityHour"
column2 = "ActivityHour"
non_unifom_dates =uniqueIDs_Comparison(hourlyCalories_merged_df, column1,
                     hourlyIntensities_merged_df,column2)


head_values <-  (head(non_unifom_dates, 5))
tail_values <- (tail(non_unifom_dates, 5))

print(head_values)
print(tail_values)

```
```{r}
# Join dataframes
CaloriesIntensities_df <- hourlyCalories_merged_df%>%inner_join(hourlyIntensities_merged_df, 
                              by=c('Id','ActivityHour'))


head(CaloriesIntensities_df, 5)


if(nrow(hourlyCalories_merged_df) == nrow(CaloriesIntensities_df) &&
   (nrow(hourlyCalories_merged_df) == nrow(hourlyIntensities_merged_df))){
  
  print("Dataframes have same number of rows")
  
}

```
```{r}
hourlySteps_merged_df <- dfs[["hourlySteps_merged_df"]]
str(hourlySteps_merged_df)

# Check if the unique Ids are similar to the ones in hourlyCalories_merged_df
# merge with  hourlyCalories_merged_df
# see the at what time are people most active 
# see correlation between steps and calories
```


```{r}
hourlySteps_merged_df <- change_to_dateTime(hourlySteps_merged_df, "ActivityHour")
```


```{r}
CaloriesIntensitiesSteps_df <- CaloriesIntensities_df%>%inner_join(
  hourlySteps_merged_df, by=c('Id', 'ActivityHour'))

head(CaloriesIntensitiesSteps_df, 5)


if(nrow(CaloriesIntensitiesSteps_df) == nrow(CaloriesIntensities_df) &&
   (nrow(CaloriesIntensitiesSteps_df) == nrow(hourlySteps_merged_df))){
  
  print("Dataframes have same number of rows")
  
}
```

```{r}
# plot a line plot see see the correlation between calories, intensity and steps
line_plot(CaloriesIntensitiesSteps_df, "Calories", "TotalIntensity")
```
```{r}
line_plot(CaloriesIntensitiesSteps_df, "Calories", "StepTotal")
```

```{r}
line_plot(CaloriesIntensitiesSteps_df, "Calories", "AverageIntensity")
```

```{r}
colnames(CaloriesIntensitiesSteps_df)
```
```{r}
custom_line_plot <- function(df, x_col, y_col, i){
    plot <- ggplot (data = df) +
      geom_line(mapping = aes(x= .data[[x_col]] , y= .data[[y_col]] )) +
      labs(title = paste(i, x_col, "and", y_col, "relationsip"))
  
  print(plot)
  
}
```
 

```{r}
CaloriesIntensitiesSteps_Daily_df <- weekDate_grouping(CaloriesIntensitiesSteps_df,
                                                       "date")
CaloriesIntensitiesSteps_Daily_df
```

```{r}
names(CaloriesIntensitiesSteps_Daily_df)
```

```{r}
IDS <- c(unique(CaloriesIntensitiesSteps_Daily_df[["Id"]]))

random_Ids = IDS[ceiling(runif((ceiling(length(IDS)*0.5)), max = length(IDS)))]

for (i in random_Ids){
  filtered_df <- CaloriesIntensitiesSteps_Daily_df %>%
    filter(Id == i)
           
  custom_line_plot(filtered_df,
            "TotalIntensity_Total","Calories_Total",i
  )
}

```
### Most people hit a point of diminishing returns between intensity of 1500 to 2000
### Advise clients to atleast hit an intensity of 1500 daily
```{r}
IDS <- c(unique(CaloriesIntensitiesSteps_Daily_df[["Id"]]))

random_Ids = IDS[ceiling(runif((ceiling(length(IDS)*0.5)), max = length(IDS)))]

for (i in random_Ids){
  filtered_df <- CaloriesIntensitiesSteps_Daily_df %>%
    filter(Id == i)
           
  custom_line_plot(filtered_df,
            "StepTotal_Total","Calories_Total",i
  )
}
```
#### different people get to the point of diminishing returns at different points there is no one fits all number of steps, however most people once they hit 2000 calories they hit a point of diminishing return

#### Reccomendation 1: Once a person has hit 2000 calories a day they can quite exercing since since the steps they are quit insignificant

#### Reccomendation 1: clients should cover the number of steps that get them to 2000 calories.

```{r}
CaloriesIntensitiesSteps_Weekly_df <-
  weekDate_grouping(CaloriesIntensitiesSteps_df, "week")
head(CaloriesIntensitiesSteps_Weekly_df, 5)
```


```{r}
IDS <- c(unique(CaloriesIntensitiesSteps_Daily_df[["Id"]]))

random_Ids = IDS[ceiling(runif((ceiling(length(IDS)*0.5)), max = length(IDS)))]

for (i in random_Ids){
  filtered_df <- CaloriesIntensitiesSteps_Weekly_df %>%
    filter(Id == i)
           
  custom_line_plot(filtered_df,
            "StepTotal_Total","Calories_Total",i
  )
}
```



```{r echo=TRUE}
IDS <- c(unique(CaloriesIntensitiesSteps_Weekly_df[["Id"]]))

random_Ids = IDS[ceiling(runif((ceiling(length(IDS)*0.5)), max = length(IDS)))]

for (i in random_Ids){
  filtered_df <- CaloriesIntensitiesSteps_Weekly_df %>%
    filter(Id == i)
           
  custom_line_plot(filtered_df,
            "TotalIntensity_Total","Calories_Total",i
  )
}

```

#### Key observation most athletes hit a point of diminishing returns after an intensity of 2500, thus they loose less caries yet the intensity is high,

#### recommendation: athletes should maintain an optimal intensity of about 2000

```{r echo=TRUE}
IDS <- c(unique(CaloriesIntensitiesSteps_Weekly_df[["Id"]]))

random_Ids = c(IDS[ceiling(runif((ceiling(length(IDS)*0.5)), max = length(IDS)))])

for (i in random_Ids){
  filtered_df <- CaloriesIntensitiesSteps_Weekly_df %>%
    filter(Id == i)
           
  custom_line_plot(filtered_df,
            "StepTotal_Total","Calories_Total", i
  )
}
```
#### Where as there in no partern in the number of steps that it takes to hit a point of diminishing returns, there's is pattern in the number of calories lost beyond which an athlete hits a point of diminishing returns which is about 15000 caries weekly


```{r}
colnames(CaloriesIntensitiesSteps_Weekly_df)
```




```{r}
# We'll perform daily Weighted averages since to get a better glimpse of the correlation since our data us concentrated at same place
# Calculate daily weighted averages


CaloriesIntensitiesSteps_daily_df <- CaloriesIntensitiesSteps_df %>%
  mutate(date = date(ActivityHour)) %>%
  group_by(date) %>%
  summarize(
    daily_calories = mean(Calories),
    daily_Intensity = mean(TotalIntensity),
    daily_Steps = mean(StepTotal)
)

head(CaloriesIntensitiesSteps_daily_df, 5)
```
```{r}
dfs <- list()


for (file in csv_files) {
  
  print(paste("Working on", file))
  
  # create df names
  # split the file name str character
  df_name <- strsplit(file, split = '\\.')[[1]] #Access the first and only string
  
  # get the first part of the string character which is basically the name 
  # without the csv extension
  df_name <- df_name[1]
  
  # concatenate the df_name with df
  df_name <- paste0(df_name, "_df") # Use paste0 for no space between parts
  
  # create full path for each file so that we can import them
  filepath <- file.path(csv_files_Dir, file)
  
  # read csv 
  df <- read.csv(filepath)
  
  #Check if df has missing value
  missing_value(df = df, file = file)
  
  # append dfs and their names
  dfs[[df_name]] <- df  # Store the data frame in the list, keyed by file path

}
```


```{r}
# Loop through the list and print key-value pairs
minutes_dfs = list()

for (key in names(dfs)) {

  if (grepl("minute", key, ignore.case = TRUE)){
    
    print(paste("working on", key))
    
    col_Name <- colnames(dfs[[key]])
    column_of_interest <- col_Name[2]
    
    df <- change_to_dateTime(dfs[[key]], column_of_interest)
    
    minutes_dfs[[key]] <- df
  }
    
}

```

```{r}
minuteSleep_merged_df <- minutes_dfs[["minuteSleep_merged_df"]]
print(head(minuteSleep_merged_df))

# See that hour are people most asleep
# change the date column to time date

unique(minuteSleep_merged_df[['value']])

```



```{r}
# Remove the key 'b' from the list
print(length(minutes_dfs))
minutes_dfs[["minuteSleep_merged_df"]] <- NULL
print(paste("Length After removing minuteSleep_merged_df",length(minutes_dfs)))
```

```{r}
for (i in minutes_dfs){
  print(i)
  main_df = minutes_dfs[[1]]
  
  column1 <- "ActivityMinute"
  column2 <- "ActivityMinute"
  
  missing_values <- uniqueIDs_Comparison(main_df, column1, minutes_dfs[[1]],
                                         column2)
}

```

```{r}
# Merge all dataframes on 'Id' and 'Date'
Minuted_Merged_df <- reduce(minutes_dfs, function(x, y) {
  merge(x, y, by = c("Id", "ActivityMinute"), all = TRUE) 
})

head(Minuted_Merged_df, 5)
```

```{r}
Sleep_Minuted_df <- minuteSleep_merged_df%>%inner_join(
  Minuted_Merged_df,
  by=c("Id" = "Id", "date" = "ActivityMinute"))

```


```{r}
head(Sleep_Minuted_df, 5)
```



```{r}
weightLogInfo_merged_df <- dfs[["weightLogInfo_merged_df"]]
head(weightLogInfo_merged_df, 5)

print(length(weightLogInfo_merged_df[['Id']]))

# see the correlation between weight and fat and and BMI
```

```{r}
ggplot (data = weightLogInfo_merged_df) +
  geom_point(mapping = aes(x = BMI , y = WeightKg), color = "blue" ) +
  geom_point(mapping = aes(x = BMI , y = WeightPounds), color = "red" ) +
  labs(title = "Weight  and BMI relation")
```

